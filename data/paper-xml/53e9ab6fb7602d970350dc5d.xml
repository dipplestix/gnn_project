<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/2007.04239v1</ns0:id>
    <ns0:updated>2020-05-31T21:52:31Z</ns0:updated>
    <ns0:published>2020-05-31T21:52:31Z</ns0:published>
    <ns0:title>A Survey on Transfer Learning in Natural Language Processing</ns0:title>
    <ns0:summary>  Deep learning models usually require a huge amount of data. However, these
large datasets are not always attainable. This is common in many challenging
NLP tasks. Consider Neural Machine Translation, for instance, where curating
such large datasets may not be possible specially for low resource languages.
Another limitation of deep learning models is the demand for huge computing
resources. These obstacles motivate research to question the possibility of
knowledge transfer using large trained models. The demand for transfer learning
is increasing as many large models are emerging. In this survey, we feature the
recent transfer learning advances in the field of NLP. We also provide a
taxonomy for categorizing different transfer learning approaches from the
literature.
</ns0:summary>
    <ns0:author>
      <ns0:name>Zaid Alyafeai</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Maged Saeed AlShaibani</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Irfan Ahmad</ns0:name>
    </ns0:author>
    <ns0:link href="http://arxiv.org/abs/2007.04239v1" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/2007.04239v1" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.CL" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.CL" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="stat.ML" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
