<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1807.06521v2</ns0:id>
    <ns0:updated>2018-07-18T11:20:08Z</ns0:updated>
    <ns0:published>2018-07-17T16:05:59Z</ns0:published>
    <ns0:title>CBAM: Convolutional Block Attention Module</ns0:title>
    <ns0:summary>  We propose Convolutional Block Attention Module (CBAM), a simple yet
effective attention module for feed-forward convolutional neural networks.
Given an intermediate feature map, our module sequentially infers attention
maps along two separate dimensions, channel and spatial, then the attention
maps are multiplied to the input feature map for adaptive feature refinement.
Because CBAM is a lightweight and general module, it can be integrated into any
CNN architectures seamlessly with negligible overheads and is end-to-end
trainable along with base CNNs. We validate our CBAM through extensive
experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.
Our experiments show consistent improvements in classification and detection
performances with various models, demonstrating the wide applicability of CBAM.
The code and models will be publicly available.
</ns0:summary>
    <ns0:author>
      <ns0:name>Sanghyun Woo</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Jongchan Park</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Joon-Young Lee</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>In So Kweon</ns0:name>
    </ns0:author>
    <ns1:comment>Accepted to ECCV 2018</ns1:comment>
    <ns0:link href="http://arxiv.org/abs/1807.06521v2" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1807.06521v2" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.CV" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.CV" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
