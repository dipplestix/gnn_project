<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1306.0543v2</ns0:id>
    <ns0:updated>2014-10-27T11:49:08Z</ns0:updated>
    <ns0:published>2013-06-03T19:16:26Z</ns0:published>
    <ns0:title>Predicting Parameters in Deep Learning</ns0:title>
    <ns0:summary>  We demonstrate that there is significant redundancy in the parameterization
of several deep learning models. Given only a few weight values for each
feature it is possible to accurately predict the remaining values. Moreover, we
show that not only can the parameter values be predicted, but many of them need
not be learned at all. We train several different architectures by learning
only a small number of weights and predicting the rest. In the best case we are
able to predict more than 95% of the weights of a network without any drop in
accuracy.
</ns0:summary>
    <ns0:author>
      <ns0:name>Misha Denil</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Babak Shakibi</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Laurent Dinh</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Marc'Aurelio Ranzato</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Nando de Freitas</ns0:name>
    </ns0:author>
    <ns0:link href="http://arxiv.org/abs/1306.0543v2" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1306.0543v2" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.NE" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="stat.ML" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
