<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1908.09355v1</ns0:id>
    <ns0:updated>2019-08-25T16:13:24Z</ns0:updated>
    <ns0:published>2019-08-25T16:13:24Z</ns0:published>
    <ns0:title>Patient Knowledge Distillation for BERT Model Compression</ns0:title>
    <ns0:summary>  Pre-trained language models such as BERT have proven to be highly effective
for natural language processing (NLP) tasks. However, the high demand for
computing resources in training such models hinders their application in
practice. In order to alleviate this resource hunger in large-scale model
training, we propose a Patient Knowledge Distillation approach to compress an
original large model (teacher) into an equally-effective lightweight shallow
network (student). Different from previous knowledge distillation methods,
which only use the output from the last layer of the teacher network for
distillation, our student model patiently learns from multiple intermediate
layers of the teacher model for incremental knowledge extraction, following two
strategies: ($i$) PKD-Last: learning from the last $k$ layers; and ($ii$)
PKD-Skip: learning from every $k$ layers. These two patient distillation
schemes enable the exploitation of rich information in the teacher's hidden
layers, and encourage the student model to patiently learn from and imitate the
teacher through a multi-layer distillation process. Empirically, this
translates into improved results on multiple NLP tasks with significant gain in
training efficiency, without sacrificing model accuracy.
</ns0:summary>
    <ns0:author>
      <ns0:name>Siqi Sun</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Yu Cheng</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Zhe Gan</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Jingjing Liu</ns0:name>
    </ns0:author>
    <ns1:comment>Accepted to EMNLP 2019</ns1:comment>
    <ns0:link href="http://arxiv.org/abs/1908.09355v1" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1908.09355v1" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.CL" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.CL" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
