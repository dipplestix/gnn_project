<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1907.13625v2</ns0:id>
    <ns0:updated>2020-01-23T09:08:54Z</ns0:updated>
    <ns0:published>2019-07-31T17:50:51Z</ns0:published>
    <ns0:title>On Mutual Information Maximization for Representation Learning</ns0:title>
    <ns0:summary>  Many recent methods for unsupervised or self-supervised representation
learning train feature extractors by maximizing an estimate of the mutual
information (MI) between different views of the data. This comes with several
immediate problems: For example, MI is notoriously hard to estimate, and using
it as an objective for representation learning may lead to highly entangled
representations due to its invariance under arbitrary invertible
transformations. Nevertheless, these methods have been repeatedly shown to
excel in practice. In this paper we argue, and provide empirical evidence, that
the success of these methods cannot be attributed to the properties of MI
alone, and that they strongly depend on the inductive bias in both the choice
of feature extractor architectures and the parametrization of the employed MI
estimators. Finally, we establish a connection to deep metric learning and
argue that this interpretation may be a plausible explanation for the success
of the recently introduced methods.
</ns0:summary>
    <ns0:author>
      <ns0:name>Michael Tschannen</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Josip Djolonga</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Paul K. Rubenstein</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Sylvain Gelly</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Mario Lucic</ns0:name>
    </ns0:author>
    <ns1:comment>ICLR 2020. Michael Tschannen and Josip Djolonga contributed equally</ns1:comment>
    <ns0:link href="http://arxiv.org/abs/1907.13625v2" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1907.13625v2" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="stat.ML" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
