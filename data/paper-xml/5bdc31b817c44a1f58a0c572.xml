<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1809.11096v2</ns0:id>
    <ns0:updated>2019-02-25T21:32:06Z</ns0:updated>
    <ns0:published>2018-09-28T15:38:49Z</ns0:published>
    <ns0:title>Large Scale GAN Training for High Fidelity Natural Image Synthesis</ns0:title>
    <ns0:summary>  Despite recent progress in generative image modeling, successfully generating
high-resolution, diverse samples from complex datasets such as ImageNet remains
an elusive goal. To this end, we train Generative Adversarial Networks at the
largest scale yet attempted, and study the instabilities specific to such
scale. We find that applying orthogonal regularization to the generator renders
it amenable to a simple "truncation trick," allowing fine control over the
trade-off between sample fidelity and variety by reducing the variance of the
Generator's input. Our modifications lead to models which set the new state of
the art in class-conditional image synthesis. When trained on ImageNet at
128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of
166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous
best IS of 52.52 and FID of 18.6.
</ns0:summary>
    <ns0:author>
      <ns0:name>Andrew Brock</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Jeff Donahue</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Karen Simonyan</ns0:name>
    </ns0:author>
    <ns0:link href="http://arxiv.org/abs/1809.11096v2" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1809.11096v2" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="stat.ML" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
