<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1712.05382v2</ns0:id>
    <ns0:updated>2018-02-23T01:35:36Z</ns0:updated>
    <ns0:published>2017-12-14T18:29:42Z</ns0:published>
    <ns0:title>Monotonic Chunkwise Attention</ns0:title>
    <ns0:summary>  Sequence-to-sequence models with soft attention have been successfully
applied to a wide variety of problems, but their decoding process incurs a
quadratic time and space cost and is inapplicable to real-time sequence
transduction. To address these issues, we propose Monotonic Chunkwise Attention
(MoChA), which adaptively splits the input sequence into small chunks over
which soft attention is computed. We show that models utilizing MoChA can be
trained efficiently with standard backpropagation while allowing online and
linear-time decoding at test time. When applied to online speech recognition,
we obtain state-of-the-art results and match the performance of a model using
an offline soft attention mechanism. In document summarization experiments
where we do not expect monotonic alignments, we show significantly improved
performance compared to a baseline monotonic attention-based model.
</ns0:summary>
    <ns0:author>
      <ns0:name>Chung-Cheng Chiu</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Colin Raffel</ns0:name>
    </ns0:author>
    <ns1:comment>ICLR camera-ready version</ns1:comment>
    <ns0:link href="http://arxiv.org/abs/1712.05382v2" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1712.05382v2" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.CL" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.CL" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="stat.ML" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
