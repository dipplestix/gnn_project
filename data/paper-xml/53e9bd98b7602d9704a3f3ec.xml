<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1405.3080v1</ns0:id>
    <ns0:updated>2014-05-13T09:45:49Z</ns0:updated>
    <ns0:published>2014-05-13T09:45:49Z</ns0:published>
    <ns0:title>Accelerating Minibatch Stochastic Gradient Descent using Stratified
  Sampling</ns0:title>
    <ns0:summary>  Stochastic Gradient Descent (SGD) is a popular optimization method which has
been applied to many important machine learning tasks such as Support Vector
Machines and Deep Neural Networks. In order to parallelize SGD, minibatch
training is often employed. The standard approach is to uniformly sample a
minibatch at each step, which often leads to high variance. In this paper we
propose a stratified sampling strategy, which divides the whole dataset into
clusters with low within-cluster variance; we then take examples from these
clusters using a stratified sampling technique. It is shown that the
convergence rate can be significantly improved by the algorithm. Encouraging
experimental results confirm the effectiveness of the proposed method.
</ns0:summary>
    <ns0:author>
      <ns0:name>Peilin Zhao</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Tong Zhang</ns0:name>
    </ns0:author>
    <ns0:link href="http://arxiv.org/abs/1405.3080v1" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1405.3080v1" rel="related" type="application/pdf" />
    <ns1:primary_category term="stat.ML" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="stat.ML" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="math.OC" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
