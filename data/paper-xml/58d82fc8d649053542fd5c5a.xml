<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1701.08734v1</ns0:id>
    <ns0:updated>2017-01-30T18:06:07Z</ns0:updated>
    <ns0:published>2017-01-30T18:06:07Z</ns0:published>
    <ns0:title>PathNet: Evolution Channels Gradient Descent in Super Neural Networks</ns0:title>
    <ns0:summary>  For artificial general intelligence (AGI) it would be efficient if multiple
users trained the same giant neural network, permitting parameter reuse,
without catastrophic forgetting. PathNet is a first step in this direction. It
is a neural network algorithm that uses agents embedded in the neural network
whose task is to discover which parts of the network to re-use for new tasks.
Agents are pathways (views) through the network which determine the subset of
parameters that are used and updated by the forwards and backwards passes of
the backpropogation algorithm. During learning, a tournament selection genetic
algorithm is used to select pathways through the neural network for replication
and mutation. Pathway fitness is the performance of that pathway measured
according to a cost function. We demonstrate successful transfer learning;
fixing the parameters along a path learned on task A and re-evolving a new
population of paths for task B, allows task B to be learned faster than it
could be learned from scratch or after fine-tuning. Paths evolved on task B
re-use parts of the optimal path evolved on task A. Positive transfer was
demonstrated for binary MNIST, CIFAR, and SVHN supervised learning
classification tasks, and a set of Atari and Labyrinth reinforcement learning
tasks, suggesting PathNets have general applicability for neural network
training. Finally, PathNet also significantly improves the robustness to
hyperparameter choices of a parallel asynchronous reinforcement learning
algorithm (A3C).
</ns0:summary>
    <ns0:author>
      <ns0:name>Chrisantha Fernando</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Dylan Banarse</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Charles Blundell</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Yori Zwols</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>David Ha</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Andrei A. Rusu</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Alexander Pritzel</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Daan Wierstra</ns0:name>
    </ns0:author>
    <ns0:link href="http://arxiv.org/abs/1701.08734v1" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1701.08734v1" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.NE" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.NE" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
