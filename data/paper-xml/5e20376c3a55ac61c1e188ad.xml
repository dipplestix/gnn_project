<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/2001.05140v2</ns0:id>
    <ns0:updated>2020-01-22T15:16:10Z</ns0:updated>
    <ns0:published>2020-01-15T05:56:59Z</ns0:published>
    <ns0:title>Graph-Bert: Only Attention is Needed for Learning Graph Representations</ns0:title>
    <ns0:summary>  The dominant graph neural networks (GNNs) over-rely on the graph links,
several serious performance problems with which have been witnessed already,
e.g., suspended animation problem and over-smoothing problem. What's more, the
inherently inter-connected nature precludes parallelization within the graph,
which becomes critical for large-sized graph, as memory constraints limit
batching across the nodes. In this paper, we will introduce a new graph neural
network, namely GRAPH-BERT (Graph based BERT), solely based on the attention
mechanism without any graph convolution or aggregation operators. Instead of
feeding GRAPH-BERT with the complete large input graph, we propose to train
GRAPH-BERT with sampled linkless subgraphs within their local contexts.
GRAPH-BERT can be learned effectively in a standalone mode. Meanwhile, a
pre-trained GRAPH-BERT can also be transferred to other application tasks
directly or with necessary fine-tuning if any supervised label information or
certain application oriented objective is available. We have tested the
effectiveness of GRAPH-BERT on several graph benchmark datasets. Based the
pre-trained GRAPH-BERT with the node attribute reconstruction and structure
recovery tasks, we further fine-tune GRAPH-BERT on node classification and
graph clustering tasks specifically. The experimental results have demonstrated
that GRAPH-BERT can out-perform the existing GNNs in both the learning
effectiveness and efficiency.
</ns0:summary>
    <ns0:author>
      <ns0:name>Jiawei Zhang</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Haopeng Zhang</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Congying Xia</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Li Sun</ns0:name>
    </ns0:author>
    <ns1:comment>10 pages</ns1:comment>
    <ns0:link href="http://arxiv.org/abs/2001.05140v2" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/2001.05140v2" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.NE" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="stat.ML" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
