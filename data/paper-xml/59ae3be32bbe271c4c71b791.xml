<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://arxiv.org/schemas/atom">
    <ns0:id>http://arxiv.org/abs/1707.04585v1</ns0:id>
    <ns0:updated>2017-07-14T03:05:43Z</ns0:updated>
    <ns0:published>2017-07-14T03:05:43Z</ns0:published>
    <ns0:title>The Reversible Residual Network: Backpropagation Without Storing
  Activations</ns0:title>
    <ns0:summary>  Deep residual networks (ResNets) have significantly pushed forward the
state-of-the-art on image classification, increasing in performance as networks
grow both deeper and wider. However, memory consumption becomes a bottleneck,
as one needs to store the activations in order to calculate gradients using
backpropagation. We present the Reversible Residual Network (RevNet), a variant
of ResNets where each layer's activations can be reconstructed exactly from the
next layer's. Therefore, the activations for most layers need not be stored in
memory during backpropagation. We demonstrate the effectiveness of RevNets on
CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification
accuracy to equally-sized ResNets, even though the activation storage
requirements are independent of depth.
</ns0:summary>
    <ns0:author>
      <ns0:name>Aidan N. Gomez</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Mengye Ren</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Raquel Urtasun</ns0:name>
    </ns0:author>
    <ns0:author>
      <ns0:name>Roger B. Grosse</ns0:name>
    </ns0:author>
    <ns0:link href="http://arxiv.org/abs/1707.04585v1" rel="alternate" type="text/html" />
    <ns0:link title="pdf" href="http://arxiv.org/pdf/1707.04585v1" rel="related" type="application/pdf" />
    <ns1:primary_category term="cs.CV" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.CV" scheme="http://arxiv.org/schemas/atom" />
    <ns0:category term="cs.LG" scheme="http://arxiv.org/schemas/atom" />
  </ns0:entry>
